{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2クラス用AUC\n",
    "from sklearn import metrics\n",
    "def auc(train, predict):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(train, predict)\n",
    "    return  metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数セーブ&ロード\n",
    "import pickle\n",
    "def varSave(filename, var):\n",
    "    fileObject = open(filename,'wb')\n",
    "    pickle.dump(var, fileObject)\n",
    "    fileObject.close()\n",
    "    return\n",
    "\n",
    "def varLoad(filename):\n",
    "    fileObject = open(filename,'rb')  \n",
    "    var = pickle.load(fileObject)  \n",
    "    fileObject.close()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot\n",
    "def onehot(target):\n",
    "    return np.eye(np.unique(target).shape[0])[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "def pca(X, dim):\n",
    "    pca = PCA(n_components=dim)\n",
    "    pca.fit(X)\n",
    "    pca.transform(X)\n",
    "    print (sum(pca.explained_variance_ratio_)) # 寄与率\n",
    "    return pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "def autoenc(X, encoding_dim, epochs):\n",
    "    input_dim = X.shape[1] # 入力次元\n",
    "    # encode, decodeの深さ\n",
    "    depth = 3\n",
    "    # エンコーダー層\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid')(encoded)\n",
    "    # デコーダー層\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    # AutoEncoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    # エンコーダーモデル\n",
    "    encoder = Model(input_img, encoded)\n",
    "    # デコーダーモデル\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder_layer = encoded_input\n",
    "    for i in range(depth):\n",
    "        decoder_layer = autoencoder.layers[i-depth](decoder_layer)\n",
    "    decoder = Model(encoded_input, decoder_layer)\n",
    "    # 最適化\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=128, shuffle=True)\n",
    "    return (encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#回帰用\n",
    "boston = datasets.load_boston()\n",
    "X=boston.data\n",
    "y=boston.target\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO\n",
    "from sklearn import linear_model\n",
    "model= linear_model.Lasso(alpha=1, fit_intercept=True) # alpha: L1係数\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#リッジ回帰\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1.0) # alpha: L2の係数\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(max_depth=5, random_state=1, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類用\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1,penalty=\"l1\") # C: 損失の係数(正則化係数の逆数)\n",
    "model.fit(X_train, y_train)\n",
    "print (model.score(X_valid, y_valid))\n",
    "print (model.predict(X_valid))\n",
    "print (model.predict_proba(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C=1, penalty=\"l1\", loss=\"squared_hinge\", dual=False) # C: 損失の係数(正則化係数の逆数)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))\n",
    "print(model._predict_proba_lr(X_valid))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel SVM\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=1,kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "lgbm_params = {\n",
    "    'objective': 'multiclass', #'xentropy',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss', #'auc',\n",
    "    'num_leaves':15,\n",
    "    # \"min_data_in_leaf\": 20,\n",
    "    # \"bagging_fraction\":1,\n",
    "    # \"bagging_freq\" : 10,\n",
    "    # \"feature_fraction\":0.98\n",
    "    \"lambda_l1\":0.1,\n",
    "    \"lambda_l2\":0.01,\n",
    "    # \"min_gain_to_split\":0.1\n",
    "    # \"max_depth\":10\n",
    "}\n",
    "model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=2000, early_stopping_rounds=20)\n",
    "print (model.predict(X_valid, num_iteration=model.best_iteration))\n",
    "print(model.feature_importance())\n",
    "lgb.plot_importance(model, ignore_zero=False,height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X_train, y_train, verbose=True)\n",
    "print(model.predict(X_valid))\n",
    "print(model.predict_proba(X_valid))\n",
    "print(model.score(X_valid, y_valid))\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "import catboost\n",
    "model = catboost.CatBoostClassifier(iterations=1000, \n",
    "                                    use_best_model=True, \n",
    "                                    eval_metric = \"HingeLoss\", #'AUC',\n",
    "                                    random_seed=1, \n",
    "                                    l2_leaf_reg=3,\n",
    "                                    depth=6,\n",
    "                                    loss_function=\"MultiClass\",#\"CrossEntropy\",\n",
    "                                    classes_count=3\n",
    "                                  )\n",
    "model.fit(X_train, y_train, \n",
    "        # cat_features=categorical_features_index, \n",
    "        eval_set=(X_valid, y_valid),\n",
    "        early_stopping_rounds=20\n",
    "        )\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネット\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation=\"relu\", input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=32, activation=\"relu\"))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, onehot(y_train), epochs=200, batch_size=256)\n",
    "model.predict_proba(X_valid, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_valid,y_valid))\n",
    "print(knn.predict_proba(X_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
