{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonの機械学習テンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ツール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2クラス用AUC\n",
    "from sklearn import metrics\n",
    "def auc(train, predict):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(train, predict)\n",
    "    return  metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 変数セーブ&ロード\n",
    "import pickle\n",
    "def varSave(filename, var):\n",
    "    fileObject = open(filename,'wb')\n",
    "    pickle.dump(var, fileObject)\n",
    "    fileObject.close()\n",
    "    return\n",
    "\n",
    "def varLoad(filename):\n",
    "    fileObject = open(filename,'rb')  \n",
    "    var = pickle.load(fileObject)  \n",
    "    fileObject.close()\n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehot\n",
    "def onehot(target):\n",
    "    return np.eye(np.unique(target).shape[0])[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 次元削減"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "def pca(X, dim):\n",
    "    pca = PCA(n_components=dim)\n",
    "    pca.fit(X)\n",
    "    pca.transform(X)\n",
    "    print (sum(pca.explained_variance_ratio_)) # 寄与率\n",
    "    return pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "def autoenc(X, encoding_dim, epochs):\n",
    "    input_dim = X.shape[1] # 入力次元\n",
    "    # encode, decodeの深さ\n",
    "    depth = 3\n",
    "    # エンコーダー層\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation='relu')(input_img)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim, activation='sigmoid')(encoded)\n",
    "    # デコーダー層\n",
    "    decoded = Dense(64, activation='relu')(encoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    # AutoEncoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    # エンコーダーモデル\n",
    "    encoder = Model(input_img, encoded)\n",
    "    # デコーダーモデル\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder_layer = encoded_input\n",
    "    for i in range(depth):\n",
    "        decoder_layer = autoencoder.layers[i-depth](decoder_layer)\n",
    "    decoder = Model(encoded_input, decoder_layer)\n",
    "    # 最適化\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=128, shuffle=True)\n",
    "    return (encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#回帰用\n",
    "boston = datasets.load_boston()\n",
    "X=boston.data\n",
    "y=boston.target\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf=KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "# for train_index, valid_index in kf.split(X):\n",
    "#     X_train, X_valid = X[train_index], X[valid_index]\n",
    "#     y_train, y_valid = y[train_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LASSO\n",
    "from sklearn import linear_model\n",
    "model= linear_model.Lasso(alpha=1, fit_intercept=True) # alpha: L1係数\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#リッジ回帰\n",
    "from sklearn.linear_model import Ridge\n",
    "model = Ridge(alpha=1.0) # alpha: L2の係数\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(max_depth=5, random_state=1, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分類用\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1,penalty=\"l1\") # C: 損失の係数(正則化係数の逆数)\n",
    "model.fit(X_train, y_train)\n",
    "print (model.score(X_valid, y_valid))\n",
    "print (model.predict(X_valid))\n",
    "print (model.predict_proba(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC(C=1, penalty=\"l1\", loss=\"squared_hinge\", dual=False) # C: 損失の係数(正則化係数の逆数)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))\n",
    "print(model._predict_proba_lr(X_valid))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel SVM\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=1,kernel='rbf')\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ランダムフォレスト\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM\n",
    "import lightgbm as lgb\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "lgbm_params = {\n",
    "    'objective': 'multiclass', #'xentropy',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss', #'auc',\n",
    "    'num_leaves':15,\n",
    "    # \"min_data_in_leaf\": 20,\n",
    "    # \"bagging_fraction\":1,\n",
    "    # \"bagging_freq\" : 10,\n",
    "    # \"feature_fraction\":0.98\n",
    "    \"lambda_l1\":0.1,\n",
    "    \"lambda_l2\":0.01,\n",
    "    # \"min_gain_to_split\":0.1\n",
    "    # \"max_depth\":10\n",
    "}\n",
    "model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval, num_boost_round=2000, early_stopping_rounds=20)\n",
    "print (model.predict(X_valid, num_iteration=model.best_iteration))\n",
    "print(model.feature_importance())\n",
    "lgb.plot_importance(model, ignore_zero=False,height=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost\n",
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "evals = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "xgb_params = {\n",
    "        'objective': 'multi:softmax',#'rmse', \n",
    "        'num_class':3,\n",
    "        'eval_metric': 'mlogloss',#'rmse',\n",
    "        \"max_depth\": 5, \n",
    "        'subsample': 0.9,\n",
    "        \"colsample_bytree\":0.9,\n",
    "        \"alpha\": 0.1,\n",
    "        \"lambda\": 0.01,\n",
    "        \"gamma\": 0.1\n",
    "}\n",
    "model=xgb.train(xgb_params, dtrain, num_boost_round=10000, early_stopping_rounds=100, evals=evals)\n",
    "print(model.predict(dvalid, ntree_limit=model.best_ntree_limit))\n",
    "print(model.get_score())\n",
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "import catboost\n",
    "model = catboost.CatBoostClassifier(iterations=1000, \n",
    "                                    use_best_model=True, \n",
    "                                    eval_metric = \"HingeLoss\", #'AUC',\n",
    "                                    random_seed=1, \n",
    "                                    l2_leaf_reg=3,\n",
    "                                    depth=6,\n",
    "                                    loss_function=\"MultiClass\",#\"CrossEntropy\",\n",
    "                                    classes_count=3\n",
    "                                  )\n",
    "model.fit(X_train, y_train, \n",
    "        # cat_features=categorical_features_index, \n",
    "        eval_set=(X_valid, y_valid),\n",
    "        early_stopping_rounds=20\n",
    "        )\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ニューラルネット\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation=\"relu\", input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=32, activation=\"relu\"))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, onehot(y_train), epochs=200, batch_size=256)\n",
    "model.predict_proba(X_valid, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-nn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_valid,y_valid))\n",
    "print(knn.predict_proba(X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 決定木: CART\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(clf.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 線形判別分析\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "lda.fit(X_train,y_train)\n",
    "print('LDA score:')\n",
    "print(lda.score(X_valid,y_valid))\n",
    "X_lda=lda.transform(X_train)\n",
    "for c in set(y_train):\n",
    "    plt.scatter(X_lda[y_train==c,0], X_lda[y_train==c,1], label=\"class\"+str(c))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-meansクラスタリング\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "print('k-means clustering:')\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 階層型クラスタリング\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, set_link_color_palette\n",
    "ward = linkage(X, method='ward', metric='euclidean')\n",
    "print('ward法:')\n",
    "print(ward) # ウォード法による結果を表示\n",
    "threshold = 0.7 * np.max(ward[:, 2]) # 適当に閾値を決める\n",
    "dendrogram(ward, color_threshold=threshold) # 階層型クラスタリング結果をプロット\n",
    "plt.show() # 表示\n",
    "clustered = fcluster(ward, threshold, criterion='distance') # 閾値で切ったときに, 各データが属するクラスタ計算\n",
    "print('clustering:')\n",
    "print(clustered)\n",
    "# wardは (n - 1) × 4 の行列になる\n",
    "# n - 1はステップ\n",
    "# 1つ目と2つ目にはどのクラスタがくっついたかっていう番号\n",
    "# 3つ目には2つのクラスタ距離\n",
    "# 4つ目には統合されたときのクラスタの要素数が入ってる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
