{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pythonの機械学習テンプレート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import datasets\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ツール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 変数セーブ&ロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def var_save(filename, var):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(var, f)\n",
    "\n",
    "def var_load(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        var = pickle.load(f)\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kf = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "# for train_index, valid_index in kf.split(X):\n",
    "#     X_train, X_valid = X[train_index], X[valid_index]\n",
    "#     y_train, y_valid = y[train_index], y[valid_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def auc(train, predict):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(train, predict)\n",
    "    return metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(target):\n",
    "    return np.eye(np.unique(target).shape[0])[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 係数最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class CoefficientOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_func,\n",
    "        initial_coef=None,\n",
    "    ):\n",
    "        self.loss_func = loss_func\n",
    "        self.initial_coef = initial_coef\n",
    "        self.status = dict()\n",
    "\n",
    "    @property\n",
    "    def coefficients(self):\n",
    "        assert \"x\" in self.coef_\n",
    "        return self.status[\"x\"]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.initial_coef is None:\n",
    "            self.initial_coef = np.ones(X.shape[1]) / X.shape[1]\n",
    "        loss_partial = partial(self._score, X=X, y=y)\n",
    "        self.status = minimize(\n",
    "            loss_partial,\n",
    "            self.initial_coef,\n",
    "            method=\"nelder-mead\",\n",
    "        )\n",
    "\n",
    "    def _score(self, coef, X, y):\n",
    "        blend = self.predict(X, coef)\n",
    "        score = self.loss_func(y, blend)\n",
    "        return score\n",
    "\n",
    "    def score(self, X, y, coef=None):\n",
    "        if coef is None:\n",
    "            coef = self.coefficients\n",
    "        return self._score(coef, X, y)\n",
    "\n",
    "    def predict(self, X, coef=None):\n",
    "        if coef is None:\n",
    "            coef = self.coefficients\n",
    "        blend = np.dot(X, coef)\n",
    "        return blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 次元削減"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X, dim):\n",
    "    pca = PCA(n_components=dim)\n",
    "    pca.fit(X)\n",
    "    pca.transform(X)\n",
    "    print(sum(pca.explained_variance_ratio_)) # 寄与率\n",
    "    return pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def autoenc(X, encoding_dim, epochs):\n",
    "    input_dim = X.shape[1] # 入力次元\n",
    "    # encode, decodeの深さ\n",
    "    depth = 3\n",
    "    # エンコーダー層\n",
    "    input_img = Input(shape=(input_dim,))\n",
    "    encoded = Dense(128, activation=\"relu\")(input_img)\n",
    "    encoded = Dense(64, activation=\"relu\")(encoded)\n",
    "    encoded = Dense(encoding_dim, activation=\"sigmoid\")(encoded)\n",
    "    # デコーダー層\n",
    "    decoded = Dense(64, activation=\"relu\")(encoded)\n",
    "    decoded = Dense(128, activation=\"relu\")(decoded)\n",
    "    decoded = Dense(input_dim, activation=\"sigmoid\")(decoded)\n",
    "    # AutoEncoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    # エンコーダーモデル\n",
    "    encoder = Model(input_img, encoded)\n",
    "    # デコーダーモデル\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder_layer = encoded_input\n",
    "    for i in range(depth):\n",
    "        decoder_layer = autoencoder.layers[i-depth](decoder_layer)\n",
    "    decoder = Model(encoded_input, decoder_layer)\n",
    "    # 最適化\n",
    "    autoencoder.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "    autoencoder.fit(X, X, epochs=epochs, batch_size=128, shuffle=True)\n",
    "    return (encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然言語のベクトル化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words=None,\n",
    "    binary=False,\n",
    "    norm=\"l2\",\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(2, 3),\n",
    "    min_df=2,\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "texts = [\"あの鳥はスズメです\", \"これはペンです\", \"あの鳥はカラスです\"]\n",
    "text_embeddings = tfidf.fit_transform(texts)\n",
    "\n",
    "# 次元削減\n",
    "# svd = TruncatedSVD(n_components=512, random_state=1)\n",
    "# vectors = svd.fit_transform(text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "\n",
    "# import ssl\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "\n",
    "texts = [\"あの鳥はスズメです\", \"これはペンです\", \"あの鳥はカラスです\"]\n",
    "vectors = embed(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "model= linear_model.Lasso(alpha=1, fit_intercept=True) # alpha: L1係数\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### リッジ回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0) # alpha: L2の係数\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(max_depth=5, random_state=1, n_estimators=100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=1,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ロジスティック回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=1, penalty=\"l1\", solver=\"liblinear\") # C: 損失の係数(正則化係数の逆数)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.score(X_valid, y_valid))\n",
    "print(model.predict(X_valid))\n",
    "print(model.predict_proba(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(C=1, penalty=\"l1\", loss=\"squared_hinge\", dual=False, max_iter=10000) # C: 損失の係数(正則化係数の逆数)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))\n",
    "print(model._predict_proba_lr(X_valid))\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(C=1, kernel=\"rbf\")\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_custom_metric(preds, data):\n",
    "    y_true = data.get_label()\n",
    "    y_pred = np.argmax(preds.reshape(3,-1), axis=0)\n",
    "    acc = np.mean(y_true == y_pred)\n",
    "    return \"custom_metric\", acc, True # 高いほうが良いときTrue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "lgbm_params = {\n",
    "    \"objective\": \"multiclass\", #\"xentropy\",\n",
    "    \"num_class\": 3,\n",
    "    \"metric\": \"multi_logloss\", #\"auc\",\n",
    "    \"num_leaves\": 15,\n",
    "    # \"min_data_in_leaf\": 20,\n",
    "    # \"bagging_fraction\": 0.9,\n",
    "    # \"bagging_freq\" : 2,\n",
    "    # \"feature_fraction\": 0.9\n",
    "    \"lambda_l1\":0.1,\n",
    "    \"lambda_l2\":0.01,\n",
    "    # \"min_gain_to_split\":0.1\n",
    "    # \"max_depth\":10\n",
    "    \"learning_rate\": 0.05, \n",
    "    \"seed\":1,\n",
    "}\n",
    "model = lgb.train(\n",
    "    lgbm_params, lgb_train, \n",
    "    valid_sets=[lgb_train, lgb_eval], \n",
    "    valid_names=[\"train\", \"valid\"],\n",
    "    num_boost_round=5000, \n",
    "#     early_stopping_rounds=50, verbose_eval=50\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(50)\n",
    "    ],\n",
    "#     feval=lgb_custom_metric\n",
    ")\n",
    "print(model.predict(X_valid, num_iteration=model.best_iteration))\n",
    "print(model.feature_importance())\n",
    "lgb.plot_importance(model, ignore_zero=False, height=0.5)\n",
    "model.best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "evals = [(dtrain, \"train\"), (dvalid, \"eval\")]\n",
    "\n",
    "xgb_params = {\n",
    "    \"objective\": \"multi:softmax\",#\"rmse\", \n",
    "    \"num_class\": 3,\n",
    "    \"eval_metric\": \"mlogloss\",#\"rmse\",\n",
    "    \"max_depth\": 5, \n",
    "    \"subsample\": 0.9,\n",
    "    \"colsample_bytree\": 0.9,\n",
    "    \"alpha\": 0.1,\n",
    "    \"lambda\": 0.01,\n",
    "    \"gamma\": 0.1\n",
    "}\n",
    "model=xgb.train(xgb_params, dtrain, num_boost_round=10000, early_stopping_rounds=100, evals=evals)\n",
    "print(model.predict(dvalid, \n",
    "#                     ntree_limit=model.best_ntree_limit,\n",
    "                    iteration_range=(0,model.best_ntree_limit)\n",
    "))\n",
    "print(model.get_score())\n",
    "xgb.plot_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "\n",
    "model = catboost.CatBoostClassifier(iterations=1000, \n",
    "                                    use_best_model=True, \n",
    "                                    eval_metric = \"HingeLoss\", # \"AUC\",\n",
    "                                    random_seed=1, \n",
    "                                    l2_leaf_reg=3,\n",
    "                                    depth=6,\n",
    "                                    loss_function=\"MultiClass\",# \"CrossEntropy\",\n",
    "                                    classes_count=3\n",
    "                                  )\n",
    "model.fit(X_train, y_train, \n",
    "        # cat_features=categorical_features_index, \n",
    "        eval_set=(X_valid, y_valid),\n",
    "        early_stopping_rounds=20\n",
    "        )\n",
    "print(model.predict(X_valid))\n",
    "print(model.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation=\"relu\", input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=32, activation=\"relu\"))\n",
    "model.add(Dense(units=3, activation=\"softmax\"))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, onehot(y_train), epochs=200, batch_size=256)\n",
    "np.argmax(model.predict(X_valid, batch_size=256), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_valid,y_valid))\n",
    "print(knn.predict_proba(X_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 決定木: CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=4)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "print(clf.score(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 線形判別分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "lda = LDA(n_components=2)\n",
    "lda.fit(X_train,y_train)\n",
    "\n",
    "print(lda.score(X_valid,y_valid))\n",
    "X_lda=lda.transform(X_train)\n",
    "for c in set(y_train):\n",
    "    plt.scatter(X_lda[y_train==c,0], X_lda[y_train==c,1], label=f\"class{c}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-meansクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=\"auto\")\n",
    "kmeans.fit(X)\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 階層型クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, set_link_color_palette\n",
    "\n",
    "ward = linkage(X, method=\"ward\", metric=\"euclidean\")\n",
    "print(\"ward:\")\n",
    "print(ward) # ウォード法による結果を表示\n",
    "threshold = 0.7 * np.max(ward[:, 2]) # 適当に閾値を決める\n",
    "dendrogram(ward, color_threshold=threshold) # 階層型クラスタリング結果をプロット\n",
    "plt.show() # 表示\n",
    "clustered = fcluster(ward, threshold, criterion=\"distance\") # 閾値で切ったときに, 各データが属するクラスタ計算\n",
    "print(\"clustering:\")\n",
    "print(clustered)\n",
    "# wardは (n - 1) × 4 の行列になる\n",
    "# n - 1はステップ\n",
    "# 1つ目と2つ目にはどのクラスタがくっついたかっていう番号\n",
    "# 3つ目には2つのクラスタ距離\n",
    "# 4つ目には統合されたときのクラスタの要素数が入ってる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
